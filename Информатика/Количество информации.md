Лекция 2. Количество информации

Мера - некоторое отображение сопоставляющее каждому числу - объект

Принципы измерения информации:
	Должно обладать свойствами меры:
		аддиктивность - значение величины целого объекта равно сумме его частей.
		положительность - не может быть отрицательным (длина, площадь, масса)
		монотонность - чем больше определенный параметр, тем больше параметр другой.
	Количество информации зависит от вероятности события.
		Если вероятность наступления события = 1, то информация о нём нулевая.
		Меньше вероятность, больше информации. Пример: непогода - сравнительно небольшая вероятность -> больше информации в рассылках.
		Получение информации снижает неопределённость при принятии решений

Аксиомы количества информации (здесь лучше читать с презентации, перенесенные формулы - невалидны, для вида)


Событие x_i наступает с вероятностью p_i = P(x_i);
Количество информации о наступлении непрерывная функция I(p_i):

1. I(p_i) >= 0, где 0 <= p_i <= 1
	При следующих вероятностях I(p_i) = 0:
	p_i = 0 - событие невозможно
	p_i = 1 - событие достоверно

2. p1 <= p2 -> I(p1) >= I(p2)
	Если вероятность первого события меньше, чем второго, то количество информации о наступлении первого - больше. Чем меньше вероятность, тем больше информации.
3. Для независимых событий (p_ij = P(x_ix_j) = p_i * p_j)
	I(p_ij) = I(p_i) + I(p_j)
Аксиомам удволетворяет функция I(p_i) = -log_n * p_i
	Где n(основание) логарифма -> размерность информации.
		n = 2 - бит
		n = 3 - трит
		n = 10 - дит
		n = e - нат


Основание логарифма определяет единицу измерение, так как
loga(p) = loga(b) * logb(p)

Энтропия как мера неопределённости

1. H(X) >= 0
2. Если одно из pi = 1, а остальные = 0, то из log1 = 0 и lim_p->0 (p * logp) = 0
следует H(X) = 0, неопределённость отсутсвует. <- если у тебя один выбор, один телевизор в магазине
3. При заданном N наибольшая энтропия будет, когда все pi = 1/N

Hmax = - N^E_i=1 * 1/N * log 1/N = -N * 1N * log 1/N = -log 1/N = log N
Док-во: bp e^x >= 1/N + x -> ln_x <= x - 1, то
H(X) - Hmax = - ( N^E_i=1 log pi) + log 1/N = log e (- (N^E_i=1 p1 ln p1 + N^E_i=1 p1 ln 1/N)

4. Если даны два независимых источника

X = ( x1 x2 ... xN)   и    Y = ( y1 ... xN )
	(p1 p2 ... pN )            ( q1 ... qN )

их совместное объединение

XY = ( x1y1 x1y2 ... xNyM )  -> 1
	 ( p1q1 p1q2 ... pNqM )

H(XY) = H(X) + H(Y)

1 -> H(XY) = - (N^E_i=1) M^E_j=1 * piqj log(piqj)

Количество информации как мера снятой неопределённости

Искомый объект в одном из квадратах, все равновероятно
```
- - -
|1|2|
- - -
|3|4|
- - -
```
Не зная, где H0 = log2 4 = 2 бит
В верхней строке - да
1 -2 возможны, нет - 3, 4
H1 = log2 2 = 1 бит
